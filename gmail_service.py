import json
import os
import logging
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build

logger = logging.getLogger(__name__)

SCOPES = ["https://www.googleapis.com/auth/gmail.modify"]


def _path(data_dir, filename):
    return os.path.join(data_dir, filename)


def get_service(data_dir):
    """Authenticate and return a Gmail API service instance."""
    token_path = _path(data_dir, "token.json")
    credentials_path = _path(data_dir, "credentials.json")
    creds = None
    if os.path.exists(token_path):
        creds = Credentials.from_authorized_user_file(token_path, SCOPES)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            if not os.path.exists(credentials_path):
                raise FileNotFoundError(
                    f"{credentials_path} not found. Download it from "
                    "Google Cloud Console (APIs & Services > Credentials)."
                )
            flow = InstalledAppFlow.from_client_secrets_file(credentials_path, SCOPES)
            creds = flow.run_local_server(port=0)
        with open(token_path, "w") as token_file:
            token_file.write(creds.to_json())
    return build("gmail", "v1", credentials=creds)


def get_profile(service):
    """Get the user's Gmail profile (includes historyId)."""
    return service.users().getProfile(userId="me").execute()


def list_messages(service, query="in:inbox", max_results=100):
    """List message IDs matching a query, paginating as needed.

    Pass max_results=None to fetch all matching messages.
    """
    messages = []
    page_token = None
    while max_results is None or len(messages) < max_results:
        batch_size = 500 if max_results is None else min(max_results - len(messages), 500)
        results = (
            service.users()
            .messages()
            .list(userId="me", q=query, maxResults=batch_size, pageToken=page_token)
            .execute()
        )
        messages.extend(results.get("messages", []))
        page_token = results.get("nextPageToken")
        if not page_token:
            break
    return messages


def list_history(service, start_history_id, label_id="INBOX"):
    """List history records since a given history ID."""
    records = []
    request = (
        service.users()
        .history()
        .list(userId="me", startHistoryId=start_history_id, labelId=label_id)
    )
    while request:
        response = request.execute()
        records.extend(response.get("history", []))
        request = (
            service.users()
            .history()
            .list_next(previous_request=request, previous_response=response)
        )
    return records


def get_message(service, message_id, format="metadata", metadata_headers=None):
    """Get a single message. Returns metadata by default."""
    kwargs = {"userId": "me", "id": message_id, "format": format}
    if metadata_headers:
        kwargs["metadataHeaders"] = metadata_headers
    return service.users().messages().get(**kwargs).execute()


def get_message_headers(service, message_id):
    """Get From, Subject, and To headers for a message."""
    msg = get_message(
        service,
        message_id,
        format="metadata",
        metadata_headers=["From", "Subject", "To"],
    )
    headers = {}
    for header in msg.get("payload", {}).get("headers", []):
        name = header["name"].lower()
        if name in ("from", "subject", "to"):
            headers[name] = header["value"]
    return headers, msg.get("labelIds", [])


def ensure_label_exists(service, label_name):
    """Create a label if it doesn't exist. Returns the label ID."""
    results = service.users().labels().list(userId="me").execute()
    for label in results.get("labels", []):
        if label["name"].lower() == label_name.lower():
            return label["id"]

    created = (
        service.users()
        .labels()
        .create(
            userId="me",
            body={
                "name": label_name,
                "labelListVisibility": "labelShow",
                "messageListVisibility": "show",
            },
        )
        .execute()
    )
    logger.info("Created label: %s (id: %s)", label_name, created["id"])
    return created["id"]


def load_scan_checkpoint(data_dir):
    """Load the set of already-processed message IDs and known senders count from disk."""
    p = _path(data_dir, "scan_checkpoint.json")
    if os.path.exists(p):
        with open(p) as f:
            data = json.load(f)
            return set(data.get("processed_ids", [])), data.get("known_senders_count", 0)
    return set(), 0


def save_scan_checkpoint(processed_ids, data_dir, known_senders_count=0):
    """Persist the set of processed message IDs and known senders count to disk."""
    with open(_path(data_dir, "scan_checkpoint.json"), "w") as f:
        json.dump({
            "processed_ids": sorted(processed_ids),
            "known_senders_count": known_senders_count,
        }, f)


def _load_recipients_cache(data_dir):
    """Load cached recipients, history ID, and scan resume index from disk."""
    p = _path(data_dir, "sent_recipients_cache.json")
    if os.path.exists(p):
        with open(p) as f:
            data = json.load(f)
            return set(data.get("recipients", [])), data.get("history_id"), data.get("resume_index", 0)
    return set(), None, 0


def _save_recipients_cache(recipients, history_id, data_dir, resume_index=0):
    """Save recipients, history ID, and scan resume index to disk."""
    with open(_path(data_dir, "sent_recipients_cache.json"), "w") as f:
        json.dump({
            "recipients": sorted(recipients),
            "history_id": history_id,
            "resume_index": resume_index,
        }, f)


def _extract_recipients_from_messages(service, messages, should_continue=None):
    """Extract recipient addresses from a list of message metadata."""
    recipients = set()
    total = len(messages)
    log_every = max(1, total // 10)
    for i, msg_meta in enumerate(messages, 1):
        if should_continue is not None and not should_continue():
            logger.info("Recipient extraction interrupted at %d/%d", i - 1, total)
            return recipients, True  # (results, interrupted)
        msg = get_message(
            service, msg_meta["id"], format="metadata", metadata_headers=["To", "Cc", "Bcc"]
        )
        for header in msg.get("payload", {}).get("headers", []):
            if header["name"].lower() in ("to", "cc", "bcc"):
                for addr in _parse_addresses(header["value"]):
                    recipients.add(addr.lower())
        if total > 10 and (i % log_every == 0 or i == total):
            logger.info("Extracting recipients: %d/%d (%.0f%%)", i, total, 100 * i / total)
    return recipients, False  # (results, interrupted)


def list_sent_recipients(service, data_dir, should_continue=None):
    """Return a sorted set of all email addresses the user has ever sent to.

    Uses incremental caching: on first run, scans all sent messages and saves
    the results. On subsequent runs, only processes new messages since the last
    cached history ID.

    should_continue: optional callable; if it returns False the scan stops early
    and saves progress for the next run.
    """
    cached_recipients, cached_history_id, resume_index = _load_recipients_cache(data_dir)

    if cached_history_id:
        # Incremental update: only fetch new sent messages since last run
        logger.info("Loading cached recipients, checking for new sent messages...")
        try:
            records = list_history(service, cached_history_id, label_id="SENT")
            new_message_ids = set()
            for record in records:
                for added in record.get("messagesAdded", []):
                    msg = added["message"]
                    if "SENT" in msg.get("labelIds", []):
                        new_message_ids.add(msg["id"])
            if new_message_ids:
                logger.info("Found %d new sent message(s)", len(new_message_ids))
                new_msgs = [{"id": mid} for mid in new_message_ids]
                new_recipients, _ = _extract_recipients_from_messages(service, new_msgs, should_continue)
                cached_recipients.update(new_recipients)
            else:
                logger.info("No new sent messages since last run")
        except Exception as e:
            if "404" in str(e):
                logger.warning("History ID expired, doing full scan")
                cached_recipients = set()
                cached_history_id = None
            else:
                raise

    if not cached_history_id:
        # Full scan: collect all sent message IDs first so we know the total,
        # then process with an accurate global progress percentage.
        logger.info("Fetching list of all sent messages...")
        all_messages = list_messages(service, query="in:sent", max_results=None)
        total = len(all_messages)
        logger.info("Found %d sent messages to process", total)

        if resume_index:
            logger.info("Resuming from message %d (skipping %d already processed)",
                        resume_index + 1, resume_index)

        for i, msg_meta in enumerate(all_messages[resume_index:], resume_index + 1):
            if should_continue is not None and not should_continue():
                logger.info("Sent recipients scan interrupted at %d/%d, progress saved", i - 1, total)
                _save_recipients_cache(cached_recipients, None, data_dir, resume_index=i - 1)
                return sorted(cached_recipients)
            msg = get_message(service, msg_meta["id"], format="metadata",
                              metadata_headers=["To", "Cc", "Bcc"])
            for header in msg.get("payload", {}).get("headers", []):
                if header["name"].lower() in ("to", "cc", "bcc"):
                    for addr in _parse_addresses(header["value"]):
                        cached_recipients.add(addr.lower())
            if i % 10 == 0 or i == total:
                logger.info("Processing sent messages: %d/%d (%.0f%%)", i, total, 100 * i / total)
                _save_recipients_cache(cached_recipients, None, data_dir, resume_index=i)

        logger.info("Sent messages scan complete: %d total", total)

    # Save final cache with current history ID
    profile = get_profile(service)
    _save_recipients_cache(cached_recipients, profile["historyId"], data_dir)

    return sorted(cached_recipients)


def _parse_addresses(header_value):
    """Extract email addresses from a header value like 'Name <email>, ...'."""
    import re
    return re.findall(r"[\w.+-]+@[\w.-]+\.\w+", header_value)


def apply_label(service, message_id, label_id):
    """Apply a label to a message."""

    service.users().messages().modify(
        userId="me", id=message_id, body={"addLabelIds": [label_id]}
    ).execute()
    logger.debug("Applied label %s to message %s", label_id, message_id)
